# -*- coding: utf-8 -*-
"""BiLSTMtest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TLICz_56SOIETdgwH7DcxjYoTyPb8BcC
"""

from google.colab import drive
drive.mount('/content/drive')

import librosa
import numpy as np
import pandas as pd
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from tqdm import tqdm
import os
import warnings
import torchtext
import torch.optim as optim
from sklearn.metrics import f1_score
import glob
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

SR = 16000
N_MFCC = 12
train_dataset = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/open/train'
N_CLASSES = 2
BATCH_SIZE = 64
N_EPOCHS = 10
LR = 1e-4
#MAX_SEQ_LEN = 200
#SEED = 15

df = pd.read_csv('/content/drive/MyDrive/dataset/TeamDeepwave/dataset/open/train.csv')
df.head()
print(df.shape)

train_df, val_df = train_test_split(df, test_size=0.3, random_state=15)
#
def load_file_paths(root_folder):
    train_path = os.path.join(root_folder, '', '*.ogg')
    print(f"Searching for files in: {train_path}")
    file_paths = glob.glob(train_path)
    return file_paths

train_files = load_file_paths(train_dataset)
print(f'Found {len(train_files)} training files.')

def get_mfcc_feature(file_paths):
    features = []
    labels = []
    for file_path in tqdm(file_paths):
        try:
            y, sr = librosa.load(file_path, sr=CONFIG.SR)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=CONFIG.N_MFCC)
            pad_width = CONFIG.MAX_SEQ_LEN - mfcc.shape[1]
            if pad_width > 0:
                mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')
            else:
                mfcc = mfcc[:, :CONFIG.MAX_SEQ_LEN]
            features.append(mfcc)

            # Assuming labels are derived from file names (e.g., 'fake' or 'real')
            if 'fake' in file_path:
                label_vector = [1, 0]  # Fake
            else:
                label_vector = [0, 1]  # Real
            labels.append(label_vector)
        except Exception as e:
            print(f"Error loading {file_path}: {e}")

    return np.array(features), np.array(labels)

train_mfcc, train_labels = get_mfcc_feature(train_files)
train_mfcc, train_labels = get_mfcc_feature(train_df)
val_mfcc, val_labels = get_mfcc_feature(val_df)

# Extract features and labels for training and validation sets
train_mfcc, train_labels = get_mfcc_feature(train_files)

# Split the dataset into training and validation sets
train_mfcc, val_mfcc, train_labels, val_labels = train_test_split(train_mfcc, train_labels, test_size=0.2, random_state=CONFIG.SEED)

# Define paths to save the data
train_mfcc_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/open/train_mfcc.npy'
train_labels_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/open/train_labels.npy'
val_mfcc_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/open/val_mfcc.npy'
val_labels_path = '/content/drive/MyDrive/dataset/TeamDeepwave/dataset/open/val_labels.npy'

# Save the data
np.save(train_mfcc_path, train_mfcc)
np.save(train_labels_path, train_labels)
np.save(val_mfcc_path, val_mfcc)
np.save(val_labels_path, val_labels)

print(f'{train_mfcc_path} {train_labels_path}')
print(f'{val_mfcc_path} {val_labels_path}')

class CustomDataset(Dataset):
    def __init__(self, mfcc, labels=None):
        self.mfcc = mfcc
        self.labels = labels

    def __len__(self):
        return len(self.mfcc)

    def __getitem__(self, index):
        if self.labels is not None:
            return self.mfcc[index], self.labels[index]
        return self.mfcc[index]

train_dataset = CustomDataset(train_mfcc, train_labels)
val_dataset = CustomDataset(val_mfcc, val_labels)

train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)

text.build_vocab(train, test, min_freq=3)
qid.build_vocab(test)
text.vocab.load_vectors(torchtext.vocab.Vectors('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'))
print(text.vocab.vectors.shape)

random.seed(1024)
train, val = train.split(split_ratio=0.7, random_state=random.getstate())

class BiLSTM(nn.Module):
    def __init__(self, pretrained_lm, padding_idx, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):
        super(BiLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.dropout = nn.Dropout(p=dropout)
        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)
        self.embedding.padding_idx = padding_idx
        if static:
            self.embedding.weight.requires_grad = False
        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,
                            hidden_size=hidden_dim,
                            num_layers=lstm_layer,
                            dropout = dropout,
                            bidirectional=True)
        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)

    def forward(self, sents):
        x = self.embedding(sents)
        x = torch.transpose(x, dim0=1, dim1=0)
        lstm_out, (h_n, c_n) = self.lstm(x)
        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))
        return y

def training(epoch, model, eval_every, loss_func, optimizer, train_iter, val_iter, early_stop=1, warmup_epoch=2):

    step = 0
    max_loss = 1e5
    no_improve_epoch = 0
    no_improve_in_previous_epoch = False
    fine_tuning = False
    train_record = []
    val_record = []
    losses = []

    for e in range(epoch):
        if e >= warmup_epoch:
            if no_improve_in_previous_epoch:
                no_improve_epoch += 1
                if no_improve_epoch >= early_stop:
                    break
            else:
                no_improve_epoch = 0
            no_improve_in_previous_epoch = True
        if not fine_tuning and e >= warmup_epoch:
            model.embedding.weight.requires_grad = True
            fine_tuning = True
        train_iter.init_epoch()
        for train_batch in iter(train_iter):
            step += 1
            model.train()
            x = train_batch.text.cuda()
            y = train_batch.target.type(torch.Tensor).cuda()
            model.zero_grad()
            pred = model.forward(x).view(-1)
            loss = loss_function(pred, y)
            losses.append(loss.cpu().data.numpy())
            train_record.append(loss.cpu().data.numpy())
            loss.backward()
            optimizer.step()
            if step % eval_every == 0:
                model.eval()
                model.zero_grad()
                val_loss = []
                for val_batch in iter(val_iter):
                    val_x = val_batch.text.cuda()
                    val_y = val_batch.target.type(torch.Tensor).cuda()
                    val_pred = model.forward(val_x).view(-1)
                    val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())
                val_record.append({'step': step, 'loss': np.mean(val_loss)})
                print('epcoh {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(
                            e, step, np.mean(losses), val_record[-1]['loss']))
                if e >= warmup_epoch:
                    if val_record[-1]['loss'] <= max_loss:
                        save(m=model, info={'step': step, 'epoch': e, 'train_loss': np.mean(losses),
                                            'val_loss': val_record[-1]['loss']})
                        max_loss = val_record[-1]['loss']
                        no_improve_in_previous_epoch = False


def save(m, info):
    torch.save(info, 'best_model.info')
    torch.save(m, 'best_model.m')

def load():
    m = torch.load('best_model.m')
    info = torch.load('best_model.info')
    return m, info

batch_size = 128
train_iter = torchtext.data.BucketIterator(dataset=train,
                                               batch_size=batch_size,
                                               sort_key=lambda x: x.text.__len__(),
                                               shuffle=True,
                                               sort=False)
val_iter = torchtext.data.BucketIterator(dataset=val,
                                             batch_size=batch_size,
                                             sort_key=lambda x: x.text.__len__(),
                                             train=False,
                                             sort=False)
model = BiLSTM(text.vocab.vectors, lstm_layer=2, padding_idx=text.vocab.stoi[text.pad_token], hidden_dim=128).cuda()
# loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([pos_w]).cuda())
loss_function = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),
                    lr=1e-3)

training(model=model, epoch=20, eval_every=500,
         loss_func=loss_function, optimizer=optimizer, train_iter=train_iter,
        val_iter=val_iter, warmup_epoch=3, early_stop=2)

model, m_info = load()
m_info

model.lstm.flatten_parameters()

model.eval()
val_pred = []
val_true = []
val_iter.init_epoch()
for val_batch in iter(val_iter):
    val_x = val_batch.text.cuda()
    val_true += val_batch.target.data.numpy().tolist()
    val_pred += torch.sigmoid(model.forward(val_x).view(-1)).cpu().data.numpy().tolist()

tmp = [0,0,0] # idx, cur, max
delta = 0
for tmp[0] in np.arange(0.1, 0.501, 0.01):
    tmp[1] = f1_score(val_true, np.array(val_pred)>tmp[0])
    if tmp[1] > tmp[2]:
        delta = tmp[0]
        tmp[2] = tmp[1]
print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))

model.eval()
model.zero_grad()
test_iter = torchtext.data.BucketIterator(dataset=test,
                                    batch_size=batch_size,
                                    sort_key=lambda x: x.text.__len__(),
                                    sort=True)
test_pred = []
test_id = []

for test_batch in iter(test_iter):
    test_x = test_batch.text.cuda()
    test_pred += torch.sigmoid(model.forward(test_x).view(-1)).cpu().data.numpy().tolist()
    test_id += test_batch.qid.view(-1).data.numpy().tolist()

sub_df =pd.DataFrame()
sub_df['qid'] = [qid.vocab.itos[i] for i in test_id]
sub_df['prediction'] = (np.array(test_pred) >= delta).astype(int)

sub_df.to_csv("submission.csv", index=False)